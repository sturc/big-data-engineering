{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# Spark libs\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, QuantileDiscretizer, VectorAssembler , Normalizer, StandardScaler, MinMaxScaler\n",
    "# helpers\n",
    "from helpers.data_prep_and_print import print_df\n",
    "from helpers.path_translation import translate_to_file_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the Imput File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = translate_to_file_string(\"../data/sales.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "       .builder\n",
    "       .appName(\"Sales Data Cleaning\")\n",
    "       .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe from csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .option(\"delimiter\", \",\") \\\n",
    "       .csv(inputFile)\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of Quantitative Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "division_indexer = StringIndexer().setInputCol(\"division\").setOutputCol(\"division_num\").fit(df)\n",
    "education_indexer = StringIndexer().setInputCol(\"level of education\").setOutputCol(\"education_num\").fit(df)\n",
    "df_indexed = education_indexer.transform(division_indexer.transform(df))\n",
    "education_encoder = OneHotEncoder().setInputCol(\"education_num\").setOutputCol(\"education_cat_vector\").setDropLast(False).fit(df_indexed)\n",
    "df_encoded = education_encoder.transform(df_indexed)\n",
    "print_df (df_encoded,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretize sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer = QuantileDiscretizer(numBuckets=10, inputCol=\"sales\", outputCol=\"sales_bucket_quantile\")\n",
    "\n",
    "df_dis = discretizer.fit(df).transform(df_encoded)\n",
    "print_df(df_dis,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build labeled point semantic vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"training level\",\"work experience\",\"salary\",\"sales_bucket_quantile\",\"division_num\",\"education_cat_vector\"]\n",
    "assembler =  VectorAssembler(outputCol=\"features\", inputCols=list(feature_cols))\n",
    "df_lp = assembler.transform(df_dis)\n",
    "print_df(df_lp,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm_features\", p=1.0)\n",
    "df_norm = normalizer.transform(df_lp)\n",
    "print_df (df_norm,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(df_norm)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "df_norm_scaled = scalerModel.transform(df_norm)\n",
    "print_df (df_norm_scaled,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"nn_features\").fit(df_lp)\n",
    "df_nn = mm_scaler.transform(df_lp)\n",
    "print_df (df_nn,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
