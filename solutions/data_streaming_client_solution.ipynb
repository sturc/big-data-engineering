{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Streaming Client \n",
    "\n",
    "**Start file streaming_server.py first by executing in a terminal ``python streaming_server.py``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "sys.path.append(\"..\")\n",
    "                \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import IntegerType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local StreamingContext with a batch interval of 10s\n",
    "spark = (SparkSession.builder\n",
    "           .appName(\"DStream Client\")\n",
    "           .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Read Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jsonSchema = StructType([ StructField(\"females\", IntegerType(), True), StructField(\"males\", IntegerType(), True),  StructField(\"age\", IntegerType(), True) ,  StructField(\"year\", IntegerType(), True)])\n",
    "\n",
    "stream = spark.readStream.format('socket').option('host', 'localhost').option('port', 9999).load()\n",
    "\n",
    "print(stream.isStreaming )  # Returns True for DataFrames that have streaming sources\n",
    "\n",
    "stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Queries on the Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream.select(from_json(stream.value,jsonSchema).alias('data'))\\\n",
    "               .select(col('data.*')).where(\"(males+females) > 100000\")\n",
    "\n",
    "\n",
    "writer = stream.writeStream.format('console').start()\n",
    "time.sleep(50)\n",
    "writer.stop() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Stream to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet file generation\n",
    "\"\"\" with tempfile.TemporaryDirectory() as cp:\n",
    "   writer = stream.writeStream.format(\"parquet\")\\\n",
    "                 .option(\"path\", \"data/census_2010.parquet\")\\\n",
    "                 .outputMode(\"append\")\\\n",
    "                 .option(\"checkpointLocation\", cp)\\\n",
    "                 .start()\n",
    "   time.sleep(50)\n",
    "   writer.stop() \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# CSV file generation \n",
    "\"\"\" with tempfile.TemporaryDirectory() as file, tempfile.TemporaryDirectory() as cp:\n",
    "    writer = stream.writeStream.format(\"csv\")\\\n",
    "                    .option(\"checkpointLocation\", cp) \\\n",
    "                    .start(file)\n",
    "    time.sleep(50)\n",
    "    writer.stop()\n",
    "    spark.read.csv(file).toPandas().to_csv(\"data/census_2010.csv\",index=False) # Save the data into a single file to a permanent location \n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
